{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OR3VFkwTRBt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8c4f4f-0021-49e2-9c14-e2771c22ae2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/Stanford-ILIAD/PantheonRL.git\n",
            "  Cloning https://github.com/Stanford-ILIAD/PantheonRL.git to /tmp/pip-req-build-d8r0rpo7\n",
            "  Running command git clone -q https://github.com/Stanford-ILIAD/PantheonRL.git /tmp/pip-req-build-d8r0rpo7\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (from pantheonrl==0.0.1) (1.1.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from pantheonrl==0.0.1) (2.9.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from pantheonrl==0.0.1) (1.13.0+cu116)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from pantheonrl==0.0.1) (2.9.1)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-1.6.2-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pantheonrl==0.0.1) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pantheonrl==0.0.1) (4.64.1)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask->pantheonrl==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask->pantheonrl==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask->pantheonrl==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask->pantheonrl==0.0.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask->pantheonrl==0.0.1) (2.0.1)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->pantheonrl==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable-baselines3->pantheonrl==0.0.1) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable-baselines3->pantheonrl==0.0.1) (3.2.2)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable-baselines3->pantheonrl==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable-baselines3->pantheonrl==0.0.1) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->pantheonrl==0.0.1) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3->pantheonrl==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3->pantheonrl==0.0.1) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3->pantheonrl==0.0.1) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3->pantheonrl==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3->pantheonrl==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable-baselines3->pantheonrl==0.0.1) (2022.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (3.19.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (0.38.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (1.51.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->pantheonrl==0.0.1) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pantheonrl==0.0.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pantheonrl==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->pantheonrl==0.0.1) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->pantheonrl==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->pantheonrl==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->pantheonrl==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->pantheonrl==0.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->pantheonrl==0.0.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->pantheonrl==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->pantheonrl==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (1.12)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (14.0.6)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (2.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (0.28.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow->pantheonrl==0.0.1) (21.3)\n",
            "Building wheels for collected packages: pantheonrl, gym\n",
            "  Building wheel for pantheonrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pantheonrl: filename=pantheonrl-0.0.1-py3-none-any.whl size=7505110 sha256=d9ec5e5d196c7bf1a72e488544554c0f893539d65c952f3f83d3942dcb360c5e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5g_hcre5/wheels/f3/d6/d7/79205258b0f7625a8dbb902b12243ac10362b6ee8968bd0daf\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=39202c715e018a87fa025e1cb970b18f2509eea59e92590ade31c3765152beb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "Successfully built pantheonrl gym\n",
            "Installing collected packages: importlib-metadata, gym, stable-baselines3, pantheonrl\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 5.1.0\n",
            "    Uninstalling importlib-metadata-5.1.0:\n",
            "      Successfully uninstalled importlib-metadata-5.1.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.21.0 importlib-metadata-4.13.0 pantheonrl-0.0.1 stable-baselines3-1.6.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.21.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Stanford-ILIAD/PantheonRL.git\n",
        "!pip install gym\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo\n",
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "h7RgZ347adQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c77d948-be0c-4865-daeb-1569a7f4e1d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo\n",
            "  Downloading PettingZoo-1.22.3-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 31.8 MB/s \n",
            "\u001b[?25hCollecting gymnasium>=0.26.0\n",
            "  Downloading gymnasium-0.27.0-py3-none-any.whl (879 kB)\n",
            "\u001b[K     |████████████████████████████████| 879 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from pettingzoo) (1.21.6)\n",
            "Collecting shimmy<1.0,>=0.1.0\n",
            "  Downloading Shimmy-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (4.4.0)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Collecting jax-jumpy>=0.2.0\n",
            "  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo) (3.11.0)\n",
            "Installing collected packages: shimmy, jax-jumpy, gymnasium-notices, gymnasium, pettingzoo\n",
            "Successfully installed gymnasium-0.27.0 gymnasium-notices-0.0.1 jax-jumpy-0.2.0 pettingzoo-1.22.3 shimmy-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.8/dist-packages (1.6.2)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (0.21.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.5.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata~=4.13 in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable_baselines3) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->stable_baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable_baselines3) (2022.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# %cd /content/drive/Shareddrives/Duong-LongWarwick/ROM\n",
        "# !python -m atari_py.import_roms .\n",
        "%cd '/content/drive/Shareddrives/Duong-KhoiBKHN/RL-Environment'\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm7iC4DQVw7m",
        "outputId": "3d0aa1b1-0610-4fbf-a025-ad65e807aff5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Shareddrives/Duong-KhoiBKHN/RL-Environment\n",
            "OvercookedAI-Environment.ipynb\tPantheonRL-MockEnvironment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.classic import tictactoe_v3 as e\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "from pantheonrl.common.agents import OnPolicyAgent\n",
        "from pantheonrl.envs.pettingzoo import PettingZooAECWrapper\n",
        "\n",
        "# We have a simple wrapper class that converts PettingZoo environments to\n",
        "# work with our framework.\n",
        "#\n",
        "# WARNING: PettingZoo environments with complex spaces may not be directly\n",
        "# compatible with our agents.\n",
        "env = PettingZooAECWrapper(e.env())\n",
        "\n",
        "\n",
        "print(env.n_players)\n",
        "# PettingZoo has many multi-player environments. To ensure that each agent\n",
        "# understands their specific observation/action space, use the getDummyEnv\n",
        "# function.\n",
        "for i in range(env.n_players - 1):\n",
        "    partner = OnPolicyAgent(PPO('MlpPolicy', env.getDummyEnv(i), verbose=1))\n",
        "\n",
        "    # The second parameter ensures that the partner is assigned to a certain\n",
        "    # player number. Forgetting this parameter would mean that all of the\n",
        "    # partner agents can be picked as `player 2`, but none of them can be\n",
        "    # picked as `player 3`.\n",
        "    env.add_partner_agent(partner, player_num=i + 1)\n",
        "\n",
        "ego = PPO('MlpPolicy', env, verbose=1)\n",
        "ego.learn(total_timesteps=50000)"
      ],
      "metadata": {
        "id": "PcuK9xU5S68H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed2cab7-3a9d-4be3-9e8b-cf7e4754cc07"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 4.11     |\n",
            "|    ep_rew_mean     | 0.32     |\n",
            "| time/              |          |\n",
            "|    fps             | 1033     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "--------------------------------------\n",
            "| name               | OnPolicyAgent |\n",
            "| rollout/           |               |\n",
            "|    ep_len_mean     | 3.36          |\n",
            "|    ep_rew_mean     | -0.394        |\n",
            "| time/              |               |\n",
            "|    iterations      | 0             |\n",
            "|    total_timesteps | 2048          |\n",
            "--------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 4.18         |\n",
            "|    ep_rew_mean          | 0.38         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 749          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0092944205 |\n",
            "|    clip_fraction        | 0.0738       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.19        |\n",
            "|    explained_variance   | -0.229       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.271        |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0187      |\n",
            "|    value_loss           | 0.597        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.35          |\n",
            "|    ep_rew_mean          | -0.333        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 1             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.008773835   |\n",
            "|    clip_fraction        | 0.0551        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.19         |\n",
            "|    explained_variance   | -0.232        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.31          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.0139       |\n",
            "|    value_loss           | 0.648         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.12        |\n",
            "|    ep_rew_mean          | 0.27        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 688         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008701233 |\n",
            "|    clip_fraction        | 0.0777      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.17       |\n",
            "|    explained_variance   | 0.0153      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.244       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.014      |\n",
            "|    value_loss           | 0.538       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.36          |\n",
            "|    ep_rew_mean          | -0.414        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 2             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.010245513   |\n",
            "|    clip_fraction        | 0.0902        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.17         |\n",
            "|    explained_variance   | 0.0312        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.33          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.0187       |\n",
            "|    value_loss           | 0.588         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.18        |\n",
            "|    ep_rew_mean          | 0.3         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 656         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008837126 |\n",
            "|    clip_fraction        | 0.0695      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.15       |\n",
            "|    explained_variance   | 0.021       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.252       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    value_loss           | 0.56        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.41          |\n",
            "|    ep_rew_mean          | -0.313        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 3             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.011462942   |\n",
            "|    clip_fraction        | 0.132         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.15         |\n",
            "|    explained_variance   | 0.0479        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.277         |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.0235       |\n",
            "|    value_loss           | 0.634         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.19        |\n",
            "|    ep_rew_mean          | 0.29        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 642         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009964527 |\n",
            "|    clip_fraction        | 0.0876      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.12       |\n",
            "|    explained_variance   | 0.0561      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.24        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    value_loss           | 0.577       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.14        |\n",
            "|    ep_rew_mean          | 0.41        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 657         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010900664 |\n",
            "|    clip_fraction        | 0.131       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.08       |\n",
            "|    explained_variance   | 0.0484      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.219       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0214     |\n",
            "|    value_loss           | 0.555       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.41          |\n",
            "|    ep_rew_mean          | -0.394        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 4             |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.011904141   |\n",
            "|    clip_fraction        | 0.127         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.1          |\n",
            "|    explained_variance   | 0.0935        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.274         |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.0204       |\n",
            "|    value_loss           | 0.611         |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 4.21       |\n",
            "|    ep_rew_mean          | 0.5        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 643        |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 22         |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01286394 |\n",
            "|    clip_fraction        | 0.143      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.01      |\n",
            "|    explained_variance   | 0.0389     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.227      |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.022     |\n",
            "|    value_loss           | 0.54       |\n",
            "----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.41          |\n",
            "|    ep_rew_mean          | -0.455        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 5             |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.008872984   |\n",
            "|    clip_fraction        | 0.0769        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.07         |\n",
            "|    explained_variance   | 0.0441        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.236         |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.0171       |\n",
            "|    value_loss           | 0.599         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 4.1          |\n",
            "|    ep_rew_mean          | 0.31         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 631          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0109561095 |\n",
            "|    clip_fraction        | 0.125        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.97        |\n",
            "|    explained_variance   | 0.0339       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.18         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0213      |\n",
            "|    value_loss           | 0.487        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.28          |\n",
            "|    ep_rew_mean          | -0.303        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 6             |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.01133617    |\n",
            "|    clip_fraction        | 0.114         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.03         |\n",
            "|    explained_variance   | 0.0628        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.22          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.0224       |\n",
            "|    value_loss           | 0.539         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.08        |\n",
            "|    ep_rew_mean          | 0.37        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 627         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012126823 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0.0662      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.205       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0232     |\n",
            "|    value_loss           | 0.511       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.35          |\n",
            "|    ep_rew_mean          | -0.333        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 7             |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.010978652   |\n",
            "|    clip_fraction        | 0.122         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.97         |\n",
            "|    explained_variance   | 0.105         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.286         |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.0221       |\n",
            "|    value_loss           | 0.587         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.08        |\n",
            "|    ep_rew_mean          | 0.4         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 623         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 32          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013028723 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | 0.0795      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.3         |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    value_loss           | 0.527       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.18          |\n",
            "|    ep_rew_mean          | -0.414        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 8             |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.011098852   |\n",
            "|    clip_fraction        | 0.114         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.9          |\n",
            "|    explained_variance   | 0.0782        |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.241         |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.0212       |\n",
            "|    value_loss           | 0.555         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.95        |\n",
            "|    ep_rew_mean          | 0.37        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 620         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 36          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012643119 |\n",
            "|    clip_fraction        | 0.131       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.8        |\n",
            "|    explained_variance   | 0.0627      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.222       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    value_loss           | 0.515       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.18        |\n",
            "|    ep_rew_mean          | 0.57        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 629         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015236076 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0.0882      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.101       |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0319     |\n",
            "|    value_loss           | 0.476       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.3           |\n",
            "|    ep_rew_mean          | -0.354        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 9             |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.012253824   |\n",
            "|    clip_fraction        | 0.129         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.87         |\n",
            "|    explained_variance   | 0.117         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.306         |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.024        |\n",
            "|    value_loss           | 0.545         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.07        |\n",
            "|    ep_rew_mean          | 0.49        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 625         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013219627 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.62       |\n",
            "|    explained_variance   | 0.0698      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.199       |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.024      |\n",
            "|    value_loss           | 0.471       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.22          |\n",
            "|    ep_rew_mean          | -0.404        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 10            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.012230985   |\n",
            "|    clip_fraction        | 0.166         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.82         |\n",
            "|    explained_variance   | 0.101         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.203         |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.026        |\n",
            "|    value_loss           | 0.534         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.03        |\n",
            "|    ep_rew_mean          | 0.35        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 623         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 46          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011455769 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | 0.105       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.157       |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0255     |\n",
            "|    value_loss           | 0.458       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.26          |\n",
            "|    ep_rew_mean          | -0.465        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 11            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.015074328   |\n",
            "|    clip_fraction        | 0.209         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.76         |\n",
            "|    explained_variance   | 0.114         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.178         |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.0322       |\n",
            "|    value_loss           | 0.489         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.92        |\n",
            "|    ep_rew_mean          | 0.49        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 620         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012627896 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.114       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.183       |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0282     |\n",
            "|    value_loss           | 0.433       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.83        |\n",
            "|    ep_rew_mean          | 0.55        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 627         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011591692 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.142       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.187       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0247     |\n",
            "|    value_loss           | 0.394       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 2.96          |\n",
            "|    ep_rew_mean          | -0.545        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 12            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.014068909   |\n",
            "|    clip_fraction        | 0.178         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.72         |\n",
            "|    explained_variance   | 0.171         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.224         |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.0305       |\n",
            "|    value_loss           | 0.471         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4           |\n",
            "|    ep_rew_mean          | 0.49        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 625         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 55          |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011880208 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0.147       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.172       |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    value_loss           | 0.36        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.14          |\n",
            "|    ep_rew_mean          | -0.475        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 13            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.016093252   |\n",
            "|    clip_fraction        | 0.188         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.66         |\n",
            "|    explained_variance   | 0.169         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.121         |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.0313       |\n",
            "|    value_loss           | 0.404         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.89         |\n",
            "|    ep_rew_mean          | 0.48         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 624          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0119637605 |\n",
            "|    clip_fraction        | 0.146        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.27        |\n",
            "|    explained_variance   | 0.163        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.19         |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.0251      |\n",
            "|    value_loss           | 0.385        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.03          |\n",
            "|    ep_rew_mean          | -0.414        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 14            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.013469138   |\n",
            "|    clip_fraction        | 0.181         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.59         |\n",
            "|    explained_variance   | 0.187         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.148         |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.0326       |\n",
            "|    value_loss           | 0.432         |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.93       |\n",
            "|    ep_rew_mean          | 0.51       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 623        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 62         |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01152578 |\n",
            "|    clip_fraction        | 0.133      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.26      |\n",
            "|    explained_variance   | 0.186      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.219      |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.025     |\n",
            "|    value_loss           | 0.418      |\n",
            "----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.04          |\n",
            "|    ep_rew_mean          | -0.455        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 15            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.01545891    |\n",
            "|    clip_fraction        | 0.227         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.53         |\n",
            "|    explained_variance   | 0.191         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.197         |\n",
            "|    n_updates            | 150           |\n",
            "|    policy_gradient_loss | -0.0381       |\n",
            "|    value_loss           | 0.47          |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.76       |\n",
            "|    ep_rew_mean          | 0.37       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 621        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 65         |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01175307 |\n",
            "|    clip_fraction        | 0.138      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.24      |\n",
            "|    explained_variance   | 0.167      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.194      |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0268    |\n",
            "|    value_loss           | 0.38       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.63        |\n",
            "|    ep_rew_mean          | 0.5         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 627         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012847154 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.173       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.148       |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0294     |\n",
            "|    value_loss           | 0.421       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 2.92          |\n",
            "|    ep_rew_mean          | -0.505        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 16            |\n",
            "|    total_timesteps      | 34816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.012615271   |\n",
            "|    clip_fraction        | 0.188         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.47         |\n",
            "|    explained_variance   | 0.174         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.163         |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.0341       |\n",
            "|    value_loss           | 0.427         |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.75       |\n",
            "|    ep_rew_mean          | 0.16       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 625        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 71         |\n",
            "|    total_timesteps      | 45056      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01415804 |\n",
            "|    clip_fraction        | 0.157      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.12      |\n",
            "|    explained_variance   | 0.186      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.148      |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0314    |\n",
            "|    value_loss           | 0.413      |\n",
            "----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 2.9           |\n",
            "|    ep_rew_mean          | -0.394        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 17            |\n",
            "|    total_timesteps      | 36864         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.013683802   |\n",
            "|    clip_fraction        | 0.215         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.43         |\n",
            "|    explained_variance   | 0.217         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.171         |\n",
            "|    n_updates            | 170           |\n",
            "|    policy_gradient_loss | -0.0362       |\n",
            "|    value_loss           | 0.45          |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.65       |\n",
            "|    ep_rew_mean          | 0.48       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 624        |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 75         |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01422408 |\n",
            "|    clip_fraction        | 0.179      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.08      |\n",
            "|    explained_variance   | 0.183      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.15       |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.0373    |\n",
            "|    value_loss           | 0.42       |\n",
            "----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 2.73          |\n",
            "|    ep_rew_mean          | -0.657        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 18            |\n",
            "|    total_timesteps      | 38912         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.015124369   |\n",
            "|    clip_fraction        | 0.194         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.245         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.142         |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -0.0348       |\n",
            "|    value_loss           | 0.454         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.74        |\n",
            "|    ep_rew_mean          | 0.55        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 623         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014602495 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.177       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.177       |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0358     |\n",
            "|    value_loss           | 0.437       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| name                    | OnPolicyAgent |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 2.62          |\n",
            "|    ep_rew_mean          | -0.687        |\n",
            "| time/                   |               |\n",
            "|    iterations           | 19            |\n",
            "|    total_timesteps      | 40960         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.014189608   |\n",
            "|    clip_fraction        | 0.188         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.33         |\n",
            "|    explained_variance   | 0.275         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 0.149         |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -0.0334       |\n",
            "|    value_loss           | 0.413         |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.57        |\n",
            "|    ep_rew_mean          | 0.69        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 622         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 82          |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017256888 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.965      |\n",
            "|    explained_variance   | 0.142       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.148       |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0324     |\n",
            "|    value_loss           | 0.359       |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f54501280d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}